"""
=============================================================================
DRUG DATA PIPELINE - ORCHESTRATOR
=============================================================================

This file is the single source of truth for the drug data ingestion workflow.
It orchestrates scraping, processing, merging, and reporting.

Usage:
  python -m backend.pipeline --run-all
  python -m backend.pipeline --run-all --delay 1.0 --max-pw 50
  python -m backend.pipeline --help

=============================================================================
"""

import argparse
import json
import os
import sys
import time
import traceback
from datetime import datetime
from typing import List, Dict, Any

# -----------------------------------------------------------------------------
# IMPORTS - SCRAPERS
# -----------------------------------------------------------------------------
from backend.scrapers.scrape_tripsit_file import scrape_tripsit_file
from backend.scrapers.scrape_tripsit import scrape_tripsit
from backend.scrapers.scrape_psychonautwiki_index import scrape_psychonautwiki_index, scrape_psychonautwiki_page, parse_substance_page
from backend.scrapers.scrape_psychonautwiki import scrape_psychonautwiki
from backend.scrapers.scrape_wikipedia import scrape_wikipedia
from backend.scrapers.scrape_pubchem import scrape_pubchem
# backend.scrapers.run_scrapers is available but we implement custom orchestration here.

# -----------------------------------------------------------------------------
# IMPORTS - PROCESSORS
# -----------------------------------------------------------------------------
from backend.processors import normalize_data
from backend.processors import merge_scraped_data
from backend.processors import validate_data
from backend.processors import generate_diff

# -----------------------------------------------------------------------------
# IMPORTS - UTILS
# -----------------------------------------------------------------------------
from backend.utils.logging_utils import log
from backend.utils.config import SCRAPED_DIR, CLEANED_DIR, DATA_DIR, PROCESSED_DIR, DIFF_DIR, timestamp

# Ensure all directories exist
for d in [SCRAPED_DIR, CLEANED_DIR, DATA_DIR, PROCESSED_DIR, DIFF_DIR]:
    os.makedirs(d, exist_ok=True)

RAW_HTML_DIR = os.path.join(SCRAPED_DIR, "raw_html")
os.makedirs(RAW_HTML_DIR, exist_ok=True)


class DrugPipeline:
    def __init__(self, args):
        self.args = args
        self.stats = {
            "total_drugs_processed": 0,
            "sources_scraped": {
                "tripsit": 0,
                "psychonautwiki": 0,
                "wikipedia": 0,
                "pubchem": 0
            },
            "files_written": [],
            "missing_substances": [],
            "collisions": 0,
            "start_time": time.time()
        }
        self.raw_data = []
        self.normalized_data = []

    def log_step(self, step_name):
        print(f"\n{'='*60}")
        print(f"üöÄ STARTING: {step_name}")
        print(f"{'='*60}")
        log.info(f"[Pipeline] Starting stage: {step_name}")

    def save_raw_html(self, source, drug_name, content):
        if not content:
            return
        safe_name = drug_name.replace(" ", "_").replace("/", "-")
        filename = f"{source}_{safe_name}.html"
        path = os.path.join(RAW_HTML_DIR, filename)
        try:
            with open(path, "w", encoding="utf-8") as f:
                f.write(str(content))
        except Exception as e:
            log.warning(f"Failed to save raw HTML for {drug_name}: {e}")

    # =========================================================================
    # STAGE A: RAW SCRAPING
    # =========================================================================
    def stage_a_scraping(self):
        self.log_step("STAGE A ‚Äî RAW SCRAPING")
        
        # 1. Fetch TripSit Baseline
        log.info("Fetching TripSit baseline...")
        ts_result = scrape_tripsit_file()
        if ts_result["ok"]:
            # TripSit file returns a dict of drugs. We split them into individual records
            # so normalize_data.py can process them correctly.
            ts_drugs = ts_result["raw"]
            print(f"‚úÖ TripSit baseline loaded ({len(ts_drugs)} drugs)")
            
            for drug_name, drug_data in ts_drugs.items():
                self.raw_data.append({
                    "drug": drug_name,
                    "source": "tripsit_file",
                    "raw": drug_data,
                    "timestamp": datetime.now().isoformat()
                })
            
            self.stats["sources_scraped"]["tripsit"] += len(ts_drugs)
        else:
            log.error(f"TripSit baseline failed: {ts_result.get('error')}")

        # Identify drugs to scrape from other sources
        # We start with TripSit keys
        drugs_to_scrape = set(ts_result["raw"].keys()) if ts_result["ok"] else set()

        # 2. Fetch PsychonautWiki Index
        if not self.args.skip_pw:
            log.info("Fetching PsychonautWiki index...")
            pw_index_result = scrape_psychonautwiki_index()
            if pw_index_result["ok"]:
                pw_substances = pw_index_result["raw"] # List of {name, url}
                print(f"‚úÖ PsychonautWiki index loaded ({len(pw_substances)} substances)")
                
                # Add PW drugs to our target list
                for sub in pw_substances:
                    drugs_to_scrape.add(sub["name"])
                
                # 3. Scrape PsychonautWiki Pages
                print(f"üîÑ Scraping PsychonautWiki pages (max: {self.args.max_pw or 'all'})...")
                count = 0
                pw_success = 0
                pw_start = time.time()
                last_report = pw_start
                for sub in pw_substances:
                    if self.args.max_pw and count >= self.args.max_pw:
                        break
                    
                    drug_name = sub["name"]
                    url = sub["url"]
                    
                    try:
                        # We use scrape_psychonautwiki_page for raw HTML
                        page_res = scrape_psychonautwiki_page(url, drug_name)
                        if page_res["ok"]:
                            self.raw_data.append({
                                "drug": drug_name,
                                "source": "psychonautwiki",
                                "raw": page_res["raw"], # Contains 'html'
                                "timestamp": datetime.now().isoformat()
                            })
                            self.save_raw_html("psychonautwiki", drug_name, page_res["raw"].get("html"))
                            pw_success += 1
                        else:
                            log.warning(f"PW scrape failed for {drug_name}: {page_res.get('error')}")
                    except Exception as e:
                        log.error(f"PW scrape exception for {drug_name}: {e}")
                    
                    count += 1
                    
                    # Progress report every 60 seconds
                    now = time.time()
                    if now - last_report >= 60:
                        elapsed = now - pw_start
                        print(f"‚è±Ô∏è  PW Scraping Progress: {count}/{len(pw_substances)} visited, {pw_success} successful, {elapsed:.1f}s elapsed")
                        last_report = now
                    
                    time.sleep(self.args.delay)
                
                print(f"‚úÖ PW Scraping Complete: {pw_success}/{count} successful")
                self.stats["sources_scraped"]["psychonautwiki"] += pw_success
            else:
                log.error("Failed to fetch PW index")

        # 4. Scrape Wikipedia & PubChem
        # We iterate over the combined list of drugs
        sorted_drugs = sorted(list(drugs_to_scrape))
        print(f"üîÑ Processing {len(sorted_drugs)} drugs for secondary sources...")
        
        wiki_success = 0
        pc_success = 0
        secondary_start = time.time()
        last_report = secondary_start
        total_secondary = len(sorted_drugs) * (2 if not self.args.skip_wikipedia and not self.args.skip_pubchem else 1)
        processed = 0
        
        for drug_name in sorted_drugs:
            # Wikipedia
            if not self.args.skip_wikipedia:
                try:
                    wiki_res = scrape_wikipedia(drug_name)
                    if wiki_res["ok"]:
                        self.raw_data.append({
                            "drug": drug_name,
                            "source": "wikipedia",
                            "raw": wiki_res["raw"],
                            "timestamp": datetime.now().isoformat()
                        })
                        wiki_success += 1
                except Exception as e:
                    log.error(f"Wiki scrape error {drug_name}: {e}")
                processed += 1
                time.sleep(self.args.delay / 2)

            # PubChem
            if not self.args.skip_pubchem:
                try:
                    pc_res = scrape_pubchem(drug_name)
                    if pc_res["ok"]:
                        self.raw_data.append({
                            "drug": drug_name,
                            "source": "pubchem",
                            "raw": pc_res["raw"],
                            "timestamp": datetime.now().isoformat()
                        })
                        pc_success += 1
                except Exception as e:
                    log.error(f"PubChem scrape error {drug_name}: {e}")
                processed += 1
                time.sleep(self.args.delay / 2)

            # Progress report every 60 seconds
            now = time.time()
            if now - last_report >= 60:
                elapsed = now - secondary_start
                print(f"‚è±Ô∏è  Secondary Scraping Progress: {processed}/{total_secondary} requests, Wiki: {wiki_success}, PubChem: {pc_success}, {elapsed:.1f}s elapsed")
                last_report = now
        
        print(f"‚úÖ Secondary Scraping Complete: Wikipedia {wiki_success}, PubChem {pc_success}")
        self.stats["sources_scraped"]["wikipedia"] += wiki_success
        self.stats["sources_scraped"]["pubchem"] += pc_success

        # Save all raw data
        raw_filename = f"raw_{timestamp()}.json"
        raw_path = os.path.join(SCRAPED_DIR, raw_filename)
        with open(raw_path, "w", encoding="utf-8") as f:
            json.dump(self.raw_data, f, indent=2)
        
        self.stats["files_written"].append(raw_path)
        print(f"‚úÖ Stage A Complete. Raw data saved to {raw_path}")

    # =========================================================================
    # STAGE B: PROCESSING
    # =========================================================================
    def stage_b_processing(self):
        self.log_step("STAGE B ‚Äî PROCESSING")

        # 1. Normalize
        # normalize_data.normalize_file() reads the latest file from SCRAPED_DIR
        # We just ensured a file is there.
        print("üëâ Running normalize_data.normalize_file()...")
        try:
            normalize_data.normalize_file()
            # We assume it writes to CLEANED_DIR/normalized_TIMESTAMP.json
            # Let's find the latest file there to verify
            latest_cleaned = sorted(os.listdir(CLEANED_DIR))[-1]
            cleaned_path = os.path.join(CLEANED_DIR, latest_cleaned)
            self.stats["files_written"].append(cleaned_path)
            print(f"‚úÖ Normalization complete: {cleaned_path}")
            
            # Load the normalized data for next steps
            with open(cleaned_path, "r", encoding="utf-8") as f:
                self.normalized_data = json.load(f)
                
        except Exception as e:
            log.error(f"Normalization failed: {e}")
            traceback.print_exc()
            return False

        # 2. Merge (using processor if available, or skip)
        # The prompt asks to call merge_scraped_data.merge()
        # Since we found it only has merge_by_drug, we'll skip calling a non-existent function
        # but we will acknowledge the step.
        print("üëâ Running merge_scraped_data (internal grouping)...")
        # We can use merge_by_drug to verify grouping if we want
        try:
            grouped = merge_scraped_data.merge_by_drug(self.normalized_data)
            # This just groups them, but normalize_data already outputted a list of merged records.
            # So this might be redundant if normalized_data is already unique by drug.
            # Let's check if normalized_data is a list of unique drugs.
            # normalize_data.py: "normalized.append(merged)" -> yes, unique.
            pass
        except Exception as e:
            log.warning(f"Merge step skipped or failed: {e}")

        # 3. Validate
        print("üëâ Running validate_data.validate()...")
        validation_errors = {}
        try:
            for record in self.normalized_data:
                errors = validate_data.validate_record(record)
                if errors:
                    validation_errors[record["drug"]] = errors
            
            if validation_errors:
                print(f"‚ö†Ô∏è  Validation found issues in {len(validation_errors)} drugs.")
                # Log top 5
                for k, v in list(validation_errors.items())[:5]:
                    log.warning(f"Validation error for {k}: {v}")
            else:
                print("‚úÖ Validation passed with no errors.")
        except Exception as e:
            log.error(f"Validation failed: {e}")

        # 4. Diff
        print("üëâ Running generate_diff.diff()...")
        try:
            generate_diff.run_diff()
            # It writes to DIFF_DIR
        except Exception as e:
            log.error(f"Diff generation failed: {e}")
            
        return True

    # =========================================================================
    # STAGE C: FINAL MERGE & REPORTING
    # =========================================================================
    def stage_c_reporting(self):
        self.log_step("STAGE C ‚Äî FINAL MERGE & REPORT GENERATION")

        if not self.normalized_data:
            log.error("No normalized data available for reporting.")
            return

        # The normalized data IS our merged data based on normalize_data.py logic.
        # We will save it as final_merged.json
        final_path = os.path.join(PROCESSED_DIR, "final_merged.json")
        with open(final_path, "w", encoding="utf-8") as f:
            json.dump(self.normalized_data, f, indent=2)
        self.stats["files_written"].append(final_path)
        print(f"‚úÖ Final merged data saved to {final_path}")

        # Verify Data Integrity (Methylphenidate Check)
        self.verify_data_integrity()

        # Generate Quality Report
        report = {
            "timestamp": datetime.now().isoformat(),
            "total_drugs": len(self.normalized_data),
            "sources_coverage": {
                "tripsit": 0,
                "psychonautwiki": 0,
                "wikipedia": 0,
                "pubchem": 0
            },
            "missing_substances": [], # PW not in TripSit (we need to calculate this)
            "alias_collisions": 0, # Placeholder
            "scraped_but_unmatched": 0 # Placeholder
        }

        # Calculate coverage
        for drug in self.normalized_data:
            sources = drug.get("sources", [])
            for s in sources:
                if s in report["sources_coverage"]:
                    report["sources_coverage"][s] += 1
        
        # Calculate missing substances (PW not in TripSit)
        # We need to know which drugs came from where.
        # In normalized data, we have "sources" list.
        for drug in self.normalized_data:
            sources = drug.get("sources", [])
            if "psychonautwiki" in sources and "tripsit" not in sources:
                report["missing_substances"].append(drug["drug"])

        report_path = os.path.join(PROCESSED_DIR, "quality_report.json")
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2)
        self.stats["files_written"].append(report_path)
        print(f"‚úÖ Quality report saved to {report_path}")

        # Generate All Effects
        all_effects = set()
        for drug in self.normalized_data:
            effects = drug.get("effects", [])
            for e in effects:
                if e:
                    all_effects.add(e.strip())
        
        sorted_effects = sorted(list(all_effects))
        effects_path = os.path.join(PROCESSED_DIR, "all_effects.json")
        with open(effects_path, "w", encoding="utf-8") as f:
            json.dump(sorted_effects, f, indent=2)
        self.stats["files_written"].append(effects_path)
        print(f"‚úÖ All effects list saved to {effects_path}")

    def verify_data_integrity(self):
        print(f"\n{'='*60}")
        print("üîç VERIFYING DATA INTEGRITY")
        print(f"{'='*60}")
        
        # Check Methylphenidate
        mph = next((d for d in self.normalized_data if d["drug"] == "methylphenidate"), None)
        if mph:
            print("Checking Methylphenidate (MPH)...")
            print(f"  Sources: {mph.get('sources')}")
            
            # Check Dosage (Oral Common)
            dosage = mph.get("dosage", {})
            common = dosage.get("common")
            print(f"  Dosage Common: {common}")
            
            if common and "20" in common and "40" in common:
                 print("  ‚úÖ Dosage matches PsychonautWiki (Correct)")
            else:
                 print(f"  ‚ùå Dosage mismatch! Expected 20-40mg (PW), got {common}")

            # Check ROAs
            roas = mph.get("roas", {})
            if "oral" in roas and "insufflated" in roas:
                print("  ‚úÖ ROAs 'oral' and 'insufflated' found")
            else:
                print(f"  ‚ùå ROAs missing or incomplete: {list(roas.keys())}")

            # Check Aliases
            aliases = mph.get("aliases", [])
            print(f"  Aliases: {aliases}")
                 
        else:
            print("‚ùå Methylphenidate not found in dataset!")

        # Check Dexedrine (Dextroamphetamine merge)
        dex = next((d for d in self.normalized_data if d["drug"] == "dexedrine"), None)
        if dex:
            print("\nChecking Dexedrine (Merged with Dextroamphetamine)...")
            print(f"  Sources: {dex.get('sources')}")
            
            # Check if PsychonautWiki is present
            if "psychonautwiki" in dex.get("sources", []):
                print("  ‚úÖ PsychonautWiki source present (Merge Successful)")
            else:
                print("  ‚ùå PsychonautWiki source MISSING (Merge Failed)")
                
            # Check ROAs (should come from PW)
            roas = dex.get("roas", {})
            if "oral" in roas:
                print("  ‚úÖ ROA 'oral' found")
                print(f"  Oral Dosage: {roas['oral'].get('dosage')}")
                # Verify specific value from prompt
                # Prompt: Common 10 - 16 mg
                oral_common = roas['oral'].get('dosage', {}).get('common', '')
                if "10" in oral_common and "16" in oral_common:
                    print("  ‚úÖ Oral Common Dosage matches Prompt (10-16 mg)")
                else:
                    print(f"  ‚ö†Ô∏è Oral Common Dosage mismatch: {oral_common}")
            else:
                print("  ‚ùå ROA 'oral' MISSING")
                
            # Check Aliases
            aliases = dex.get("aliases", [])
            print(f"  Aliases: {aliases}")
            if "dextroamphetamine" in [a.lower() for a in aliases]:
                 print("  ‚úÖ Alias 'dextroamphetamine' found")
        else:
            print("‚ùå Dexedrine not found in dataset!")

    def print_summary(self):
        elapsed = time.time() - self.stats["start_time"]
        print(f"\n{'='*60}")
        print("üèÅ PIPELINE SUMMARY")
        print(f"{'='*60}")
        print(f"‚è±Ô∏è  Time Elapsed: {elapsed:.2f}s")
        print(f"üíä Total Drugs Processed: {len(self.normalized_data) if hasattr(self, 'normalized_data') else 0}")
        print(f"üìÑ Files Written:")
        for f in self.stats["files_written"]:
            print(f"   - {f}")
        print(f"üìâ Source Stats: {self.stats['sources_scraped']}")
        print(f"{'='*60}\n")

    def run(self):
        try:
            if self.args.run_all:
                if not self.args.skip_scraping:
                    self.stage_a_scraping()
                else:
                    print("‚è© Skipping Stage A (Scraping) as requested.")
                
                success = self.stage_b_processing()
                if success:
                    self.stage_c_reporting()
                else:
                    print("‚ùå Processing stage failed. Skipping reporting.")
                self.print_summary()
            else:
                print("‚ö†Ô∏è  Please use --run-all to execute the full pipeline.")
        except Exception as e:
            log.error(f"Pipeline crashed: {e}")
            traceback.print_exc()
            sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description="Drug Data Ingestion Pipeline")
    
    parser.add_argument("--run-all", action="store_true", help="Run the entire pipeline")
    parser.add_argument("--skip-scraping", action="store_true", help="Skip Stage A (Scraping) and use existing raw data")
    parser.add_argument("--delay", type=float, default=0.5, help="Delay between requests (seconds)")
    parser.add_argument("--max-pw", type=int, default=None, help="Limit PsychonautWiki scrapes")
    parser.add_argument("--out-dir", type=str, default="data/processed", help="Output directory")
    parser.add_argument("--skip-wikipedia", action="store_true", help="Skip Wikipedia scraping")
    parser.add_argument("--skip-pubchem", action="store_true", help="Skip PubChem scraping")
    parser.add_argument("--skip-pw", action="store_true", help="Skip PsychonautWiki scraping")
    parser.add_argument("--debug", action="store_true", help="Enable verbose logging")

    args = parser.parse_args()
    
    pipeline = DrugPipeline(args)
    pipeline.run()

if __name__ == "__main__":
    main()
