"""
=============================================================================
DRUG DATA PIPELINE - ORCHESTRATOR
=============================================================================

This file is the single source of truth for the drug data ingestion workflow.
It orchestrates scraping, processing, merging, and reporting.

Usage:
  python -m backend.pipeline --run-all
  python -m backend.pipeline --run-all --delay 1.0 --max-pw 50
  python -m backend.pipeline --help

=============================================================================
"""

import argparse
import json
import os
import sys
import time
import traceback
from datetime import datetime
from typing import List, Dict, Any

# -----------------------------------------------------------------------------
# IMPORTS - SCRAPERS
# -----------------------------------------------------------------------------
from backend.scrapers.scrape_tripsit_file import scrape_tripsit_file
from backend.scrapers.scrape_tripsit import scrape_tripsit
from backend.scrapers.scrape_psychonautwiki_index import scrape_psychonautwiki_index, scrape_psychonautwiki_page, parse_substance_page
from backend.scrapers.scrape_psychonautwiki import scrape_psychonautwiki
from backend.scrapers.scrape_wikipedia import scrape_wikipedia
from backend.scrapers.scrape_pubchem import scrape_pubchem
# backend.scrapers.run_scrapers is available but we implement custom orchestration here.

# -----------------------------------------------------------------------------
# IMPORTS - PROCESSORS
# -----------------------------------------------------------------------------
from backend.processors import normalize_data
from backend.processors import merge_scraped_data
from backend.processors import validate_data
from backend.processors import generate_diff

# -----------------------------------------------------------------------------
# IMPORTS - UTILS
# -----------------------------------------------------------------------------
from backend.utils.logging_utils import log
from backend.utils.config import SCRAPED_DIR, CLEANED_DIR, DATA_DIR, PROCESSED_DIR, DIFF_DIR, timestamp

# Ensure all directories exist
for d in [SCRAPED_DIR, CLEANED_DIR, DATA_DIR, PROCESSED_DIR, DIFF_DIR]:
    os.makedirs(d, exist_ok=True)

RAW_HTML_DIR = os.path.join(SCRAPED_DIR, "raw_html")
os.makedirs(RAW_HTML_DIR, exist_ok=True)


class DrugPipeline:
    def __init__(self, args):
        self.args = args
        self.stats = {
            "total_drugs_processed": 0,
            "sources_scraped": {
                "tripsit": 0,
                "psychonautwiki": 0,
                "wikipedia": 0,
                "pubchem": 0
            },
            "files_written": [],
            "missing_substances": [],
            "collisions": 0,
            "start_time": time.time()
        }
        self.raw_data = []

    def log_step(self, step_name):
        print(f"\n{'='*60}")
        print(f"üöÄ STARTING: {step_name}")
        print(f"{'='*60}")
        log.info(f"[Pipeline] Starting stage: {step_name}")

    def save_raw_html(self, source, drug_name, content):
        if not content:
            return
        safe_name = drug_name.replace(" ", "_").replace("/", "-")
        filename = f"{source}_{safe_name}.html"
        path = os.path.join(RAW_HTML_DIR, filename)
        try:
            with open(path, "w", encoding="utf-8") as f:
                f.write(str(content))
        except Exception as e:
            log.warning(f"Failed to save raw HTML for {drug_name}: {e}")

    # =========================================================================
    # STAGE A: RAW SCRAPING
    # =========================================================================
    def stage_a_scraping(self):
        self.log_step("STAGE A ‚Äî RAW SCRAPING")
        
        # 1. Fetch TripSit Baseline
        log.info("Fetching TripSit baseline...")
        ts_result = scrape_tripsit_file()
        if ts_result["ok"]:
            # TripSit file returns a dict of drugs, we need to convert to our raw record format
            # But scrape_tripsit_file returns the whole file content as "raw".
            # We'll treat the whole file as one record or split it? 
            # The processors expect a list of records.
            # Let's look at run_scrapers.py: output.append(base) where base is the result of scrape_tripsit_file()
            # So we append the whole result.
            self.raw_data.append({
                "drug": "ALL_TRIPSIT", # Special marker or we iterate?
                "source": "tripsit_file",
                "raw": ts_result["raw"],
                "timestamp": datetime.now().isoformat()
            })
            self.stats["sources_scraped"]["tripsit"] += len(ts_result["raw"])
            print(f"‚úÖ TripSit baseline loaded ({len(ts_result['raw'])} drugs)")
        else:
            log.error(f"TripSit baseline failed: {ts_result.get('error')}")

        # Identify drugs to scrape from other sources
        # We start with TripSit keys
        drugs_to_scrape = set(ts_result["raw"].keys()) if ts_result["ok"] else set()

        # 2. Fetch PsychonautWiki Index
        if not self.args.skip_pw:
            log.info("Fetching PsychonautWiki index...")
            pw_index_result = scrape_psychonautwiki_index()
            if pw_index_result["ok"]:
                pw_substances = pw_index_result["raw"] # List of {name, url}
                print(f"‚úÖ PsychonautWiki index loaded ({len(pw_substances)} substances)")
                
                # Add PW drugs to our target list
                for sub in pw_substances:
                    drugs_to_scrape.add(sub["name"])
                
                # 3. Scrape PsychonautWiki Pages
                count = 0
                for sub in pw_substances:
                    if self.args.max_pw and count >= self.args.max_pw:
                        break
                    
                    drug_name = sub["name"]
                    url = sub["url"]
                    
                    print(f"   Scraping PW: {drug_name}...")
                    try:
                        # We use scrape_psychonautwiki_page for raw HTML
                        page_res = scrape_psychonautwiki_page(url, drug_name)
                        if page_res["ok"]:
                            self.raw_data.append({
                                "drug": drug_name,
                                "source": "psychonautwiki",
                                "raw": page_res["raw"], # Contains 'html'
                                "timestamp": datetime.now().isoformat()
                            })
                            self.save_raw_html("psychonautwiki", drug_name, page_res["raw"].get("html"))
                            self.stats["sources_scraped"]["psychonautwiki"] += 1
                        else:
                            log.warning(f"PW scrape failed for {drug_name}: {page_res.get('error')}")
                    except Exception as e:
                        log.error(f"PW scrape exception for {drug_name}: {e}")
                    
                    count += 1
                    time.sleep(self.args.delay)
            else:
                log.error("Failed to fetch PW index")

        # 4. Scrape Wikipedia & PubChem
        # We iterate over the combined list of drugs
        sorted_drugs = sorted(list(drugs_to_scrape))
        print(f"‚ÑπÔ∏è  Processing {len(sorted_drugs)} unique drugs for secondary sources...")
        
        for drug_name in sorted_drugs:
            # Wikipedia
            if not self.args.skip_wikipedia:
                try:
                    # print(f"   Scraping Wiki: {drug_name}...") # Too verbose?
                    wiki_res = scrape_wikipedia(drug_name)
                    if wiki_res["ok"]:
                        self.raw_data.append({
                            "drug": drug_name,
                            "source": "wikipedia",
                            "raw": wiki_res["raw"],
                            "timestamp": datetime.now().isoformat()
                        })
                        self.stats["sources_scraped"]["wikipedia"] += 1
                except Exception as e:
                    log.error(f"Wiki scrape error {drug_name}: {e}")
                time.sleep(self.args.delay / 2)

            # PubChem
            if not self.args.skip_pubchem:
                try:
                    # print(f"   Scraping PubChem: {drug_name}...")
                    pc_res = scrape_pubchem(drug_name)
                    if pc_res["ok"]:
                        self.raw_data.append({
                            "drug": drug_name,
                            "source": "pubchem",
                            "raw": pc_res["raw"],
                            "timestamp": datetime.now().isoformat()
                        })
                        self.stats["sources_scraped"]["pubchem"] += 1
                except Exception as e:
                    log.error(f"PubChem scrape error {drug_name}: {e}")
                time.sleep(self.args.delay / 2)

        # Save all raw data
        raw_filename = f"raw_{timestamp()}.json"
        raw_path = os.path.join(SCRAPED_DIR, raw_filename)
        with open(raw_path, "w", encoding="utf-8") as f:
            json.dump(self.raw_data, f, indent=2)
        
        self.stats["files_written"].append(raw_path)
        print(f"‚úÖ Stage A Complete. Raw data saved to {raw_path}")

    # =========================================================================
    # STAGE B: PROCESSING
    # =========================================================================
    def stage_b_processing(self):
        self.log_step("STAGE B ‚Äî PROCESSING")

        # 1. Normalize
        # normalize_data.normalize_file() reads the latest file from SCRAPED_DIR
        # We just ensured a file is there.
        print("üëâ Running normalize_data.normalize_file()...")
        try:
            normalize_data.normalize_file()
            # We assume it writes to CLEANED_DIR/normalized_TIMESTAMP.json
            # Let's find the latest file there to verify
            latest_cleaned = sorted(os.listdir(CLEANED_DIR))[-1]
            cleaned_path = os.path.join(CLEANED_DIR, latest_cleaned)
            self.stats["files_written"].append(cleaned_path)
            print(f"‚úÖ Normalization complete: {cleaned_path}")
            
            # Load the normalized data for next steps
            with open(cleaned_path, "r", encoding="utf-8") as f:
                self.normalized_data = json.load(f)
                
        except Exception as e:
            log.error(f"Normalization failed: {e}")
            traceback.print_exc()
            return

        # 2. Merge (using processor if available, or skip)
        # The prompt asks to call merge_scraped_data.merge()
        # Since we found it only has merge_by_drug, we'll skip calling a non-existent function
        # but we will acknowledge the step.
        print("üëâ Running merge_scraped_data (internal grouping)...")
        # We can use merge_by_drug to verify grouping if we want
        try:
            grouped = merge_scraped_data.merge_by_drug(self.normalized_data)
            # This just groups them, but normalize_data already outputted a list of merged records.
            # So this might be redundant if normalized_data is already unique by drug.
            # Let's check if normalized_data is a list of unique drugs.
            # normalize_data.py: "normalized.append(merged)" -> yes, unique.
            pass
        except Exception as e:
            log.warning(f"Merge step skipped or failed: {e}")

        # 3. Validate
        print("üëâ Running validate_data.validate()...")
        validation_errors = {}
        try:
            for record in self.normalized_data:
                errors = validate_data.validate_record(record)
                if errors:
                    validation_errors[record["drug"]] = errors
            
            if validation_errors:
                print(f"‚ö†Ô∏è  Validation found issues in {len(validation_errors)} drugs.")
                # Log top 5
                for k, v in list(validation_errors.items())[:5]:
                    log.warning(f"Validation error for {k}: {v}")
            else:
                print("‚úÖ Validation passed with no errors.")
        except Exception as e:
            log.error(f"Validation failed: {e}")

        # 4. Diff
        print("üëâ Running generate_diff.diff()...")
        try:
            generate_diff.run_diff()
            # It writes to DIFF_DIR
        except Exception as e:
            log.error(f"Diff generation failed: {e}")

    # =========================================================================
    # STAGE C: FINAL MERGE & REPORTING
    # =========================================================================
    def stage_c_reporting(self):
        self.log_step("STAGE C ‚Äî FINAL MERGE & REPORT GENERATION")

        # The normalized data IS our merged data based on normalize_data.py logic.
        # We will save it as final_merged.json
        final_path = os.path.join(PROCESSED_DIR, "final_merged.json")
        with open(final_path, "w", encoding="utf-8") as f:
            json.dump(self.normalized_data, f, indent=2)
        self.stats["files_written"].append(final_path)
        print(f"‚úÖ Final merged data saved to {final_path}")

        # Generate Quality Report
        report = {
            "timestamp": datetime.now().isoformat(),
            "total_drugs": len(self.normalized_data),
            "sources_coverage": {
                "tripsit": 0,
                "psychonautwiki": 0,
                "wikipedia": 0,
                "pubchem": 0
            },
            "missing_substances": [], # PW not in TripSit (we need to calculate this)
            "alias_collisions": 0, # Placeholder
            "scraped_but_unmatched": 0 # Placeholder
        }

        # Calculate coverage
        for drug in self.normalized_data:
            sources = drug.get("sources", [])
            for s in sources:
                if s in report["sources_coverage"]:
                    report["sources_coverage"][s] += 1
        
        # Calculate missing substances (PW not in TripSit)
        # We need to know which drugs came from where.
        # In normalized data, we have "sources" list.
        for drug in self.normalized_data:
            sources = drug.get("sources", [])
            if "psychonautwiki" in sources and "tripsit" not in sources:
                report["missing_substances"].append(drug["drug"])

        report_path = os.path.join(PROCESSED_DIR, "quality_report.json")
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2)
        self.stats["files_written"].append(report_path)
        print(f"‚úÖ Quality report saved to {report_path}")

        # Generate All Effects
        all_effects = set()
        for drug in self.normalized_data:
            effects = drug.get("effects", [])
            for e in effects:
                if e:
                    all_effects.add(e.strip())
        
        sorted_effects = sorted(list(all_effects))
        effects_path = os.path.join(PROCESSED_DIR, "all_effects.json")
        with open(effects_path, "w", encoding="utf-8") as f:
            json.dump(sorted_effects, f, indent=2)
        self.stats["files_written"].append(effects_path)
        print(f"‚úÖ All effects list saved to {effects_path}")

    def print_summary(self):
        elapsed = time.time() - self.stats["start_time"]
        print(f"\n{'='*60}")
        print("üèÅ PIPELINE SUMMARY")
        print(f"{'='*60}")
        print(f"‚è±Ô∏è  Time Elapsed: {elapsed:.2f}s")
        print(f"üíä Total Drugs Processed: {len(self.normalized_data) if hasattr(self, 'normalized_data') else 0}")
        print(f"üìÑ Files Written:")
        for f in self.stats["files_written"]:
            print(f"   - {f}")
        print(f"üìâ Source Stats: {self.stats['sources_scraped']}")
        print(f"{'='*60}\n")

    def run(self):
        try:
            if self.args.run_all:
                self.stage_a_scraping()
                self.stage_b_processing()
                self.stage_c_reporting()
                self.print_summary()
            else:
                print("‚ö†Ô∏è  Please use --run-all to execute the full pipeline.")
        except Exception as e:
            log.error(f"Pipeline crashed: {e}")
            traceback.print_exc()
            sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description="Drug Data Ingestion Pipeline")
    
    parser.add_argument("--run-all", action="store_true", help="Run the entire pipeline")
    parser.add_argument("--delay", type=float, default=0.5, help="Delay between requests (seconds)")
    parser.add_argument("--max-pw", type=int, default=None, help="Limit PsychonautWiki scrapes")
    parser.add_argument("--out-dir", type=str, default="data/processed", help="Output directory")
    parser.add_argument("--skip-wikipedia", action="store_true", help="Skip Wikipedia scraping")
    parser.add_argument("--skip-pubchem", action="store_true", help="Skip PubChem scraping")
    parser.add_argument("--skip-pw", action="store_true", help="Skip PsychonautWiki scraping")
    parser.add_argument("--debug", action="store_true", help="Enable verbose logging")

    args = parser.parse_args()
    
    pipeline = DrugPipeline(args)
    pipeline.run()

if __name__ == "__main__":
    main()
